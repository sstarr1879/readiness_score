{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import researchpy as rp\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "from numpy import exp\n",
    "import scipy.stats as ss\n",
    "import statsmodels.api as sa\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd, MultiComparison\n",
    "from scipy import stats #for kruskal wallis ANOVA\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_frame = pd.read_excel('Readiness Score.xlsx', sheet_name='Data')\n",
    "\n",
    "# transform to be exponential\n",
    "metrics_frame['red_score'] = exp(metrics_frame['Total Readiness Score'])\n",
    "metrics_frame['red_score'] = boxcox(metrics_frame['red_score'], 0)\n",
    "\n",
    "# power transform\n",
    "#data = boxcox(data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_unit = metrics_frame.groupby(['ACTUAL UNIT']).count().reset_index()\n",
    "count_unit[['ACTUAL UNIT', 'Total Readiness Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = count_unit.plot.bar(x='ACTUAL UNIT',y='Full Name', rot=0)\n",
    "ax.tick_params(axis='x', labelrotation=45)\n",
    "ax.set_title('Count by ACTUAL UNIT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Readiness Score Overall Stats:')\n",
    "print(rp.summary_cont(metrics_frame['Total Readiness Score']))\n",
    "print('Total Readiness Score Grouped by ACTUAL UNIT Stats:')\n",
    "print(rp.summary_cont(metrics_frame['Total Readiness Score'].groupby(metrics_frame['ACTUAL UNIT'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_frame['Total Readiness Score'].hist()\n",
    "pl.suptitle(\"Total Readiness Score Distribution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics_frame['Total Readiness Score'].hist(by=metrics_frame['ACTUAL UNIT'], \n",
    " #                                          figsize=(20,20))\n",
    "metrics_frame['red_score'].hist(by=metrics_frame['ACTUAL UNIT'], \n",
    "                                           figsize=(20,20))\n",
    "    #plt.figure(figsize=(20,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsets(groupa, groupb):\n",
    "    print('comparing %s and %s:' % (groupa, groupb))\n",
    "    unit_loc_a = groupa\n",
    "    unit_a_df = metrics_frame.loc[metrics_frame['ACTUAL UNIT'] == unit_loc_a]\n",
    "    print('Group %s with average score %s and size %s' % (groupa, \n",
    "                                                          unit_a_df['Total Readiness Score'].mean(),\n",
    "                                                          len(unit_a_df)))\n",
    "    unit_loc_b = groupb\n",
    "    unit_b_df = metrics_frame.loc[metrics_frame['ACTUAL UNIT'] == unit_loc_b]\n",
    "    print('Group %s with average score %s and size %s' % (groupb, \n",
    "                                                          unit_b_df['Total Readiness Score'].mean(),\n",
    "                                                          len(unit_b_df)))\n",
    "    return unit_a_df, unit_b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_t_test(unit_a, unit_b):\n",
    "    t2, p2 = stats.ttest_ind(unit_a['Total Readiness Score'], unit_b['Total Readiness Score'])\n",
    "    print('T value: {}'.format(t2))\n",
    "    print('P value: {}'.format(p2))\n",
    "    return t2,p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_frame['ACTUAL UNIT'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = []\n",
    "p_list = []\n",
    "group_list = []\n",
    "group1_vals = []\n",
    "group2_vals = []\n",
    "group1_mean = []\n",
    "group2_mean = []\n",
    "group1_size = []\n",
    "group2_size = []\n",
    "group1_std = []\n",
    "group2_std = []\n",
    "combinations = list(it.combinations(metrics_frame['ACTUAL UNIT'].unique(),2))\n",
    "for group1, group2 in combinations: \n",
    "    group1_df, group2_df = create_subsets(group1, group2)\n",
    "    t, p = run_t_test(group1_df, group2_df)\n",
    "    t_list.append(t)\n",
    "    p_list.append(p)\n",
    "    group_list.append((group1,group2)),\n",
    "    group1_vals.append(group1),\n",
    "    group2_vals.append(group2),\n",
    "    group1_mean.append(group1_df['Total Readiness Score'].mean()),\n",
    "    group2_mean.append(group2_df['Total Readiness Score'].mean()),\n",
    "    group1_size.append(len(group1_df)),\n",
    "    group2_size.append(len(group2_df)),\n",
    "    group1_std.append(group1_df['Total Readiness Score'].std()),\n",
    "    group2_std.append(group2_df['Total Readiness Score'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_frame = pd.DataFrame({'group':group_list,'t_value':t_list,'p_value':p_list,\n",
    "                            'group1':group1_vals,'group1_size':group1_size,'group1_mean':group1_mean,\n",
    "                            'group1_std':group1_std,'group2':group2_vals,'group2_size':group2_size,\n",
    "                             'group2_mean':group2_mean, 'group2_std':group2_std,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxlot = t_test_frame.boxplot(column=(['t_value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_test_frame.to_csv(r'C:\\Users\\sarah\\Documents\\python\\readiness_score_output\\t_test_frame_CITY_SECTION.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics_frame['CITY/SECTION'].nunique()\n",
    "#metrics_frame['trs'] = metrics_frame['Total Readiness Score']\n",
    "metrics_frame['trs'] = metrics_frame['red_score']\n",
    "\n",
    "#metrics_frame['city_section'] = metrics_frame['CITY/SECTION']\n",
    "metrics_frame['actual_unit'] = metrics_frame['ACTUAL UNIT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Way ANOVA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ols('trs ~ actual_unit', data=metrics_frame).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aov_table = sm.stats.anova_lm(mod, type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aov_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_table(aov):\n",
    "    aov['mean_sq'] = aov[:]['sum_sq']/aov[:]['df']\n",
    "    \n",
    "    aov['eta_sq'] = aov[:-1]['sum_sq']/sum(aov['sum_sq'])\n",
    "    \n",
    "    aov['omega_sq'] = (aov[:-1]['sum_sq']-(aov[:-1]['df']*aov['mean_sq'][-1]))/(sum(aov['sum_sq'])+aov['mean_sq'][-1])\n",
    "    \n",
    "    cols = ['sum_sq', 'df', 'mean_sq', 'F', 'PR(>F)', 'eta_sq', 'omega_sq']\n",
    "    aov = aov[cols]\n",
    "    return aov\n",
    "\n",
    "anova_table(aov_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check ANOVA Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when working with linear regression and ANOVA models, the assumptions pertain to the residuals \n",
    "#and not the variables themselves. \n",
    "#Using Statsmodels, we can use the diagnostics that is already provided\n",
    "mod.diagn\n",
    "#Jarque-Bera (jb; jbpv is p-value) tests the assumption of normality\n",
    "#Omnibus (omni; omnipv is p-value) tests the assumption of homogeneity of variance\n",
    "#Condition Number (condno) assess multicollinearity. \n",
    "#Condition Number values over 20 are indicative of multicollinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homogeneity of Variance Check using Levene's\n",
    "#Levene’s test for homogeneity of variance is not significant \n",
    "#which indicates that the groups have equal variances.\n",
    "stats.levene(metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'BOSTON'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'CINO(IAAG)'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'CMD GRP'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'HHD'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'SUPPORT'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'MNT VIEW'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'AUSTIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.residplot(metrics_frame.actual_unit, metrics_frame.trs.astype('float'), lowess=True, color=\"g\")\n",
    "mod.resid.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Normality check\n",
    "#tested on the residuals as a whole which is how the diagnostic information provided by statsmodels tests\n",
    "#the residuals. One could use the Jarque-Bera test provided, or one could use Shapiro or others.\n",
    "stats.shapiro(mod.resid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA Post-Hoc Analysis\n",
    "\n",
    "### Tukey All pairwise comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = MultiComparison(metrics_frame['trs'],metrics_frame['actual_unit'])\n",
    "tukey_result = mc.tukeyhsd(alpha=0.10)\n",
    "print(tukey_result)\n",
    "print('Unique actual_unit groups: {}'.format(mc.groupsunique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non parametric Kruskal-Wallis ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, p = ss.kruskal(metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'BOSTON'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'CINO(IAAG)'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'CMD GRP'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'HHD'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'SUPPORT'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'MNT VIEW'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'AUSTIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"h: {}\".format(H))\n",
    "print(\"p: {}\".format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to the assumption that H has a chi square distribution, the number of samples in each group \n",
    "#must not be too small.\n",
    "#A typical rule is that each sample must have at least 5 measurements.\n",
    "#This test is an alternative to One way ANOVA when the data violates the \n",
    "#assumptions of normal distribution and when the sample size is too small.\n",
    "\n",
    "stats.kruskal(metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'BOSTON'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'CINO(IAAG)'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'CMD GRP'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'HHD'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'SUPPORT'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'MNT VIEW'],\n",
    "             metrics_frame['Total Readiness Score'][metrics_frame['ACTUAL UNIT'] == 'AUSTIN'])\n",
    "\n",
    "#if there are 3 or more independent comparison groups with 5 or more observations in each group \n",
    "#then the test statistic H approximates a chi-square distribution with k-1 degree of freedom. \n",
    "#Therefore, in such a case, you can find the critical value of the test in the chi-square distribution \n",
    "#table for critical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Parametric Post Hoc Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sp.posthoc_nemenyi(metrics_frame, val_col='Total Readiness Score', group_col='ACTUAL UNIT')\n",
    "sp.posthoc_dunn(metrics_frame, val_col='Total Readiness Score', group_col='ACTUAL UNIT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unspervised Clustering \n",
    "\n",
    "* Feature correlation\n",
    "* Silhoutte score calculation\n",
    "* k means clustering\n",
    "* cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics_frame.columns #return the columns available in the dataframe\n",
    "feature_columns = ['ACTUAL UNIT','PHA Due.1', 'Dental Due.1', 'Eval Due.1', 'APFT Due.1', 'DD93 Due.1',\n",
    "'SGLV Due.1', 'PRR Due.1', 'Medical Score', 'Eval Score',\n",
    "'Soldier Skill Score', 'Admin Score', 'Total Readiness Score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_frame_subset = metrics_frame[feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw metrics\n",
    "corr = metrics_frame_subset.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_frame_subset_grouped = metrics_frame_subset.groupby('ACTUAL UNIT')[['PHA Due.1', 'Dental Due.1', 'Eval Due.1', 'APFT Due.1', 'DD93 Due.1',\n",
    "'SGLV Due.1', 'PRR Due.1', 'Medical Score', 'Eval Score',\n",
    "'Soldier Skill Score', 'Admin Score']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_frame_subset_grouped.plot(subplots=True,layout=(4,5), figsize=(20,20))\n",
    "#plt.tight_layout()\n",
    "#plt.tick_params(labelcolor='none', top='off', bottom='off', left='off', right='off')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr = metrics_frame_subset_grouped.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(metrics_frame_subset_grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = StandardScaler().fit_transform(metrics_frame_subset_grouped[metrics_frame_subset_grouped.columns[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_n_clusters = list (range(2,7))\n",
    "print (\"Number of clusters from 2 to 9: \\n\", range_n_clusters)\n",
    "silhouette_score_values=list()\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    preds = clusterer.fit_predict(x)\n",
    "    centers = clusterer.cluster_centers_\n",
    "\n",
    "    score = silhouette_score(x, preds)\n",
    "    print(\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))\n",
    "    silhouette_score_values.append(silhouette_score(x, preds,metric='euclidean', sample_size=None, random_state=None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(range_n_clusters, silhouette_score_values)\n",
    "plt.clf()\n",
    "title_obj =plt.title(\"Silhouette score values vs Numbers of Clusters \")\n",
    "plt.setp(title_obj , color='w')         #set the color of title to white\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(111)\n",
    "#ax = fig.add_subplot(111)\n",
    "#ax.set_xlabel('n-clusters')\n",
    "#ax.set_ylabel('Silhouette Score')\n",
    "plt.rc_context({'ytick.color':'white','xtick.color':'white'})\n",
    "#ax.xaxis.label.set_color('white')\n",
    "#ax.yaxis.label.set_color('white')\n",
    "\n",
    "#ax.tick_params(axis='x', colors='white')\n",
    "#ax.tick_params(axis='y', colors='white')\n",
    "plt.plot(range_n_clusters, silhouette_score_values)\n",
    "\n",
    "plt.show()\n",
    "Optimal_NumberOf_Components=range_n_clusters[silhouette_score_values.index(max(silhouette_score_values))]\n",
    "print(\"Optimal number of components is:\")\n",
    "print(Optimal_NumberOf_Components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = KMeans(n_clusters=Optimal_NumberOf_Components)\n",
    "preds = clusterer.fit_predict(metrics_frame_subset_grouped[metrics_frame_subset_grouped.columns[1:]])\n",
    "metrics_frame_subset_grouped['cluster'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "#targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "#colors = ['r', 'g', 'b','y']\n",
    "#for target, color in zip(targets,colors):\n",
    "#indicesToKeep = finalDf['week'] == target\n",
    "#finalDf.loc[finalDf.week, 'principal component 1']\n",
    "            #   , finalDf.loc[finalDf.week, 'principal component 2']\n",
    "\n",
    "LABEL_COLOR_MAP = {0 : 'r',\n",
    "               1 : 'b',\n",
    "               2:'g',\n",
    "               3:'y'\n",
    "               }\n",
    "\n",
    "label_color = [LABEL_COLOR_MAP[l] for l in metrics_frame_subset_grouped.cluster]\n",
    "ax.xaxis.label.set_color('white')\n",
    "ax.yaxis.label.set_color('white')\n",
    "ax.title.set_color('white')\n",
    "ax.tick_params(axis='x', colors='white')\n",
    "ax.tick_params(axis='y', colors='white')\n",
    "ax.scatter(principalDf['principal component 1']\n",
    "               ,principalDf['principal component 2']\n",
    "               , s = 50, c=label_color)\n",
    "#ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize = (20,20))\n",
    "metrics_frame_subset_grouped.boxplot(by='cluster', figsize=(20,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
